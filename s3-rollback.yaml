Description: Roll back an Amazon S3 bucket to a given point in time. See https://github.com/aws-solutions-library-samples/sample-s3-rollback-tool/ for details. AWS Solutions Library reference SO9651

Parameters:

  BucketName:
    Type: String
    Description: The Amazon S3 bucket name to be rolled back to a point in time. Must have versioning enabled.
    AllowedPattern: '[a-z0-9][a-z0-9.-]{1,61}[a-z0-9]'
    ConstraintDescription: Bucket name must not be blank
    Default: "bucket"

  TimeStamp:
    Type: String
    Description: The date and time to roll back to in the UTC timezone, in ISO <yyyy-mm-ddThh:mm:ss> format. For example 2025-08-30T02:00:00.
    AllowedPattern: '\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}((\+\d{2}:\d{2})|Z)?'
    ConstraintDescription: Please enter a timestamp in ISO format, e.g. 2025-08-30T02:00:00
    Default: "2025-08-30T02:00:00"

  Prefix:
    Type: String
    Description: If you want to limit recovery to a specific prefix, specify it here. Leave blank to roll back the entire bucket. This value will be used to select from available S3 Inventories, and is required when there is no S3 Inventory of the whole bucket.
    Default: ""

  ExecutionMode:
    Type: String
    Description: <Bucket Rollback> mode adds delete markers to new objects created after the timestamp, removes delete markers for existing objects deleted since the timestamp, and copies older object versions that have been overwritten since the point in time. <Delete Marker Removal> mode simply removes any delete markers created after the timestamp, for objects where a delete marker is the current version, down to the next regular object version. <Copy to Bucket> mode copies all objects present at the timestamp to a different bucket. See https://github.com/aws-solutions-library-samples/sample-s3-rollback-tool/ for more details.
    AllowedValues:
      - "Bucket Rollback"
      - "Delete Marker Removal"
      - "Copy to Bucket"
    ConstraintDescription: Please choose either "Bucket Rollback", "Delete Marker Removal" or "Copy to Bucket".
    Default: "Bucket Rollback"
  
  StorageClass:
    Type: String
    Description: The S3 storage class to copy object versions into. As always with S3, if in doubt, use Intelligent-Tiering. See https://docs.aws.amazon.com/AmazonS3/latest/userguide/sc-howtoset.html for more information.
    AllowedValues:
      - STANDARD
      - INTELLIGENT_TIERING
      - STANDARD_IA
      - GLACIER_IR
      - GLACIER
      - DEEP_ARCHIVE
    ConstraintDescription: Please choose a valid Amazon S3 storage class, e.g INTELLIGENT_TIERING.
    Default: "INTELLIGENT_TIERING"

  KMSKey:
    Type: String
    Description:  If object versions should be created using a KMS encryption key, specify it here. Leave blank for SSE-S3. Permissions to KMS keys are not updated by this tool - see the documentation.
    ConstraintDescription: Specify a valid KMS key ARN. If using SSE-S3, leave blank.
    AllowedPattern: '(arn:.+:kms:.+:\d{12}:(key|alias)/.+)?'
    Default: ""

  StartS3BatchOperationsJobs:
    Type: String
    Description: The tool creates S3 Batch Operations jobs to revert changes, but by default it will not start the jobs. If you are working with a test dataset and do not need to validate the operations, change this to "YES".
    AllowedValues:
      - "YES"
      - "NO"
    ConstraintDescription: Please select YES or NO.
    Default: "NO"

  UseMetadata:
    Type: String
    Description: If S3 Metadata is enabled on the bucket, should the tool attempt to use it?
    AllowedValues:
      - "YES"
      - "NO"
    ConstraintDescription: Please select YES or NO.
    Default: "YES"

  InventoryCSVLocation:
    Type: String
    Description: S3 location of a CSV inventory file to use instead of S3 Inventory or Metadata (e.g. s3://bucket/prefix/inventory.csv). Can be generated with https://github.com/aws-samples/sample-s3-listobjectversions-to-csv
    AllowedPattern: "^(|s3://[a-zA-Z0-9._-]+(/[^/]+)*\\.csv)$"
    ConstraintDescription: Please enter an S3 location ending in .csv e.g. s3://mybucket/prefix/inventory.csv
    Default: ""

  DestinationBucket:
    Type: String
    Description: S3 bucket to use as the destination, if using "Copy to Bucket" mode.
    AllowedPattern: '([a-z0-9][a-z0-9.-]{1,61}[a-z0-9])?'
    Default: ""

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "Amazon S3 rollback tool"
        Parameters:
          - BucketName
          - TimeStamp
          - Prefix
          - ExecutionMode
          - StartS3BatchOperationsJobs
      -        
        Label:
          default: "Optional parameters"
        Parameters:
          - UseMetadata
          - StorageClass
          - KMSKey
          - InventoryCSVLocation
          - DestinationBucket

    ParameterLabels:
        BucketName:
          default: Bucket
        Prefix:
          default: Prefix
        TimeStamp:
          default: Timestamp
        ExecutionMode:
          default: Execution Mode
        UseMetadata:
          default: Use S3 Metadata tables
        StorageClass:
          default: Copy to storage class
        KMSKey:
          default: Copy using KMS key
        StartS3BatchOperationsJobs:
          default: Start S3 Batch Operations jobs
        InventoryCSVLocation:
          default: Specify CSV inventory
        DestinationBucket:
          default: Destination Bucket

Conditions:
  CreateInventory: !Equals
    - !Ref InventoryCSVLocation
    - ""
  
  CreateInventoryFromMetadata: !And
    - !Equals
      - !Ref InventoryCSVLocation
      - ""
    - !Equals
      - !Ref UseMetadata
      - "YES"

  UseInventoryCSV: !Not
    - !Equals [ !Ref InventoryCSVLocation, "" ]    
 
  WantKMS: !Not
    - !Equals [ !Ref KMSKey, "" ]
    
  WantRollback: !Equals
    - !Ref ExecutionMode
    - "Bucket Rollback"

  WantDMRemoval: !Equals
    - !Ref ExecutionMode
    - "Delete Marker Removal"

  WantCopy: !Equals
    - !Ref ExecutionMode
    - "Copy to Bucket"

  WantRollbackOrCopy: !Or
    - !Equals
      - !Ref ExecutionMode
      - "Bucket Rollback"
    - !Equals
      - !Ref ExecutionMode
      - "Copy to Bucket"

  WantRollbackOrDMRemoval: !Or
    - !Equals
      - !Ref ExecutionMode
      - "Bucket Rollback"
    - !Equals
      - !Ref ExecutionMode
      - "Delete Marker Removal"
      
  UseMetadata: !Equals
    - !Ref UseMetadata
    - "YES"

Rules:

  CheckForDestinationBucket:
    RuleCondition: !Equals
      - !Ref ExecutionMode
      - "Copy to Bucket"
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref DestinationBucket
          - ""
        AssertDescription: Destination Bucket must be specified if using Copy to Bucket mode.

Resources:

  BucketCleaner:
    Type: "AWS::Lambda::Function"
    Properties:
      Runtime: python3.13
      MemorySize: 1024
      Timeout: 900
      Handler: "index.handler"
      Role: !GetAtt BucketCleanerRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import boto3
            
          def handler(event, context):
            
            if event["RequestType"] != "Delete":
              status = {
                "Status" : f'Exiting on RequestType = {event["RequestType"]}',
                "Event" : event
              }
              cfnresponse.send(event, context, cfnresponse.SUCCESS, status)
              return
            
            bucket = event["ResourceProperties"]["Bucket"]
            s3 = boto3.client("s3")
            
            try:
              response = s3.head_bucket(Bucket = bucket)
            except Exception as e:
              status = {
                "Status" : f'Exiting due to error = {e}',
                "Event" : event
              }
              cfnresponse.send(event, context, cfnresponse.SUCCESS, status)
              return
            
            count = 0
            total = 0
            
            for p in s3.get_paginator('list_objects_v2').paginate(Bucket = bucket):
            
              keys = [ {"Key" : c["Key"] } for c in p.get("Contents", []) ]
                
              if keys:
                total += len(keys)                
                response = s3.delete_objects(Bucket = bucket, Delete = { "Objects" : keys })
                count += len(response.get("Deleted", []))
                
            status = { "Status" : f'Deleted {count} / {total} keys.' }
            cfnresponse.send(event, context, cfnresponse.SUCCESS, status)

  BucketCleanerRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
              - Effect: Allow
                Action:
                  - "s3:ListBucket"
                  - "s3:DeleteObject"
                Resource:
                  - !GetAtt AthenaResultsBucket.Arn
                  - !Sub "${AthenaResultsBucket.Arn}/*"
                  
  CheckVersioning:
    Type: "AWS::Lambda::Function"
    Properties:
      Runtime: python3.13
      MemorySize: 128
      Timeout: 60
      Handler: "index.handler"
      Role: !GetAtt CheckVersioningRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import boto3
          import botocore.exceptions
          
          def handler(event, context):
          
            if event["RequestType"] != "Create":
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              return
            
            s3 = boto3.client("s3")
            
            try:
            
              for b in ["Source", "Destination"]:
              
                bucket = event["ResourceProperties"].get(f"{b}Bucket")
                if not bucket:
                  continue 
              
                # Ensure bucket has versioning enabled.
              
                response = s3.get_bucket_versioning(Bucket = bucket)
          
                if response.get("Status") != "Enabled":
                  print("Valid bucket but versioning not enabled.")
                  reason = f"{b} Bucket '{bucket}' does not have versioning enabled."
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)
                  return
                  
                print("Valid bucket and versioning enabled.")
              
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              return          
            
            except botocore.exceptions.ClientError as error:
            
              if error.response['Error']['Code'] == "NoSuchBucket":
                reason = f"{b} Bucket '{bucket}' does not exist."
            
              else:
                reason = f"Error checking versioning for {b} bucket '{bucket}': {error.response['Error']['Message']}"
            
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)
              return

  CheckVersioningRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
              - Effect: Allow
                Action:
                  - "s3:GetBucketVersioning"
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3:::${BucketName}"
                  - !If [ WantCopy, !Sub "arn:${AWS::Partition}:s3:::${DestinationBucket}", !Ref "AWS::NoValue" ]
  
  AthenaResultsBucket:
    Type: "AWS::S3::Bucket"

  GlueDatabase:
    Type: "AWS::Glue::Database"
    Properties:
      CatalogId: !Ref "AWS::AccountId"
      DatabaseInput:
        Name: !Join [ "_", [ "s3_rollback_db", !Join [ "_", !Split [ "-", !Select [ 2, !Split [ "/", !Ref "AWS::StackId" ] ] ] ] ] ]
        Description: !Sub "Holds the tables for the S3 Rollback solution in CloudFormation Stack '${AWS::StackName}'"
        
  GluePermissions:
    Type: AWS::LakeFormation::PrincipalPermissions
    Properties:
      Principal:
        DataLakePrincipalIdentifier: !GetAtt QueryExecutorRole.Arn
      Resource:
        Database:
          CatalogId: !Ref "AWS::AccountId"
          Name: !Ref GlueDatabase
      Permissions:
        - "CREATE_TABLE"
      PermissionsWithGrantOption: []

  S3TablesPermissions:
    Type: AWS::LakeFormation::Permissions
    Condition: CreateInventoryFromMetadata
    Properties:
      DataLakePrincipal:
        DataLakePrincipalIdentifier: !GetAtt QueryExecutorRole.Arn
      Permissions: ["SELECT"]
      Resource:
        TableResource:
          CatalogId: !GetAtt ExecuteMetadataFinder.Catalog
          DatabaseName: !GetAtt ExecuteMetadataFinder.NameSpace
          TableWildcard: {}
       
  AthenaWorkGroup:
    Type: "AWS::Athena::WorkGroup"
    Properties:
      Name: !Join [ "_", [ "s3_rollback_wg", !Join [ "_", !Split [ "-", !Select [ 2, !Split [ "/", !Ref "AWS::StackId" ] ] ] ] ] ]
      Description: !Sub "WorkGroup for the S3 Rollback '${AWS::StackName}' CloudFormation Stack"
      RecursiveDeleteOption: true
      WorkGroupConfiguration: 
        EnforceWorkGroupConfiguration: false 

  MetadataFinder:
    Type: "AWS::Lambda::Function"
    Condition: CreateInventoryFromMetadata
    Properties:
      Runtime: python3.13
      MemorySize: 1024
      Timeout: 60
      Handler: "index.handler"
      Role: !GetAtt MetadataFinderRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import json
          import boto3
          import botocore.exceptions
          
          def handler(event, context):
          
            if event["RequestType"] != "Create":
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              return
            
            bucket = event["ResourceProperties"]["Bucket"]
            
            sts = boto3.client("sts")
            account_id = sts.get_caller_identity()["Account"]
              
            response_data = {
              "Catalog" : account_id,
              "NameSpace" : "default",
              "Journal" : "",
              "Inventory" : ""
            }
            
            use_metadata = event["ResourceProperties"]["UseMetadata"]
            
            if (use_metadata != "YES"):
                cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
                return
                
            try:
            
              s3 = boto3.client("s3")
              response = s3.get_bucket_metadata_configuration(Bucket = bucket)["GetBucketMetadataConfigurationResult"]["MetadataConfigurationResult"]
              
              reasons = []
              
              state = response["JournalTableConfigurationResult"]["TableStatus"]
              if state != "ACTIVE":
                reasons.append(f"Journal table configuration state is {state}.")
              
              state = response["InventoryTableConfigurationResult"]["ConfigurationState"]
              if state != "ENABLED":
                reasons.append(f"Inventory table configuration state is {state}.")
              else:
                state = response["InventoryTableConfigurationResult"]["TableStatus"]
                if state != "ACTIVE":
                  reasons.append(f"Inventory table status is {state}.")
              
              if reasons:
                reason = "Could not read Metadata. " + " ".join(reasons)
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)         
                return
              
              response_data = {
                "Catalog" : f"{account_id}:s3tablescatalog/aws-s3",
                "NameSpace" : response["DestinationResult"]["TableNamespace"],
                "Journal" : response["JournalTableConfigurationResult"]["TableName"],
                "Inventory" : response["InventoryTableConfigurationResult"]["TableName"]
              }
                
              # Now need to check inventory or journal isn't KMS encrypted.
              s3tables = boto3.client("s3tables")
              
              for t in ["Inventory", "Journal"]:
              
                table_name = response_data[t]
                enc_response = s3tables.get_table_encryption(
                  tableBucketARN = response["DestinationResult"]["TableBucketArn"],
                  namespace = response_data["NameSpace"],
                  name = response_data[t]
                )["encryptionConfiguration"]
                
                if enc_response["sseAlgorithm"] == "aws:kms":
                
                  reason = f"Cannot read S3 Metadata. {t} table is encrypted with KMS key {enc_response['kmsKeyArn']}"
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)         
                  return
              
              cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
              return
            
            except botocore.exceptions.ClientError as error:
              
              if error.response['Error']['Code'] == "MetadataConfigurationNotFound":
                cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
                return
              
              if error.response['Error']['Code'] == "NoSuchBucket":
                reason = f"Bucket '{bucket}' does not exist."
              else:
                reason = f"Querying Metadata for bucket '{bucket}' failed: {error.response['Error']['Code']} {error.response['Error']['Message']}"
              
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)         
              return

  MetadataFinderRole:
    Type: "AWS::IAM::Role"
    Condition: CreateInventoryFromMetadata
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
              - Effect: Allow
                Action:
                  - "s3:GetBucketMetadataTableConfiguration"
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3:::${BucketName}"
              - Effect: Allow
                Action:
                  - "s3tables:GetTableEncryption"
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3tables:${AWS::Region}:${AWS::AccountId}:bucket/aws-s3/table/*"

  InventoryFinder:
    Type: "AWS::Lambda::Function"
    Condition: CreateInventory
    Properties:
      Runtime: python3.13
      MemorySize: 1024
      Timeout: 60
      Handler: "index.handler"
      Role: !GetAtt InventoryFinderRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import json
          import re
          import boto3
          import botocore.exceptions
          from datetime import datetime
          from datetime import timezone
          
          def handler(event, context):
          
            if event["RequestType"] != "Create":
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              return
            
            # Have we already found a Metadata config?
            # If so, return dummy inventory data so we don't break creation of subsequent queries / IAM Roles.
            
            if event["ResourceProperties"]["NameSpace"] != "default":
                  
              response_data = {
                "DateTime" : "",
                "InventoryLocation" : "",
                "InventoryArn" : event["ResourceProperties"]["ResultsBucketArn"],
                "Serde" : "",
                "InventorySource" : "S3 Metadata"
              }
            
              cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
              return
            
            fields = set(["LastModifiedDate", "StorageClass", "ETag", "ChecksumAlgorithm", "ObjectOwner"])
            inv = response_data = None
            invs = []
            
            bucket = event["ResourceProperties"]["Bucket"]
            
            try:
            
              timestamp = datetime.fromisoformat(event["ResourceProperties"]["TimeStamp"])
              if timestamp.tzinfo is None:
                timestamp = timestamp.replace(tzinfo=timezone.utc)
              
              scope = event["ResourceProperties"]["Prefix"]
              
              session = boto3.Session()
              s3 = session.client("s3")
              
              response = s3.list_bucket_inventory_configurations(Bucket = bucket)
            
            except ValueError as error:
              
              reason = f"Error parsing timestamp '{event["ResourceProperties"]["TimeStamp"]}': {error}"
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)
              return
            
            except botocore.exceptions.ClientError as error:
              
              if error.response['Error']['Code'] == "NoSuchBucket":
                reason = f"Bucket '{bucket}' does not exist."
              
              else:
                reason = f"Error listing inventory configurations for bucket '{bucket}': {error.response['Error']['Message']}"
              
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)
              return
            
            while True:
            
              bad_prefix = False
              
              for inv in response.get("InventoryConfigurationList", []):
            
                # We need an inventory in ORC / Parquet format, that has all object versions,
                # contains the fields we're interested in, and meets the prefix scope the user asked for.
                if (inv["Destination"]["S3BucketDestination"]["Format"] in ["ORC", "Parquet"] and
                  inv["IncludedObjectVersions"] == "All" and fields.issubset(set(inv["OptionalFields"]))):
                
                  # Leading or trailing slashes in prefix used by Inventory breaks Athena.
                  prefix = inv["Destination"]["S3BucketDestination"].get("Prefix", "")
                  if prefix and (prefix[0] == "/" or prefix[-1] == "/"):
                    bad_prefix = True
                  else:
                    invs.append(inv)
            
              if not response["IsTruncated"]:
                break
              
              response = s3.list_bucket_inventory_configurations(Bucket = bucket,
                ContinuationToken = response["NextContinuationToken"])
            
            # Now look through all the inventory configurations we found,
            # and choose the most recent one.
            
            serde_map = {
              "ORC": "org.apache.hadoop.hive.ql.io.orc.OrcSerde",
              "Parquet" : "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe"
            }
            
            bad_scope = False
            for inv in invs:
              
              id = inv["Id"]
              d = inv["Destination"]["S3BucketDestination"]
              inv_bucket = d["Bucket"].split(":")[5]
              bucket_loc = f'{d["Prefix"]}/' if d.get("Prefix") else ""
              prefix = f'{bucket_loc}{bucket}/{inv["Id"]}/hive/dt='
              
              filter = inv.get("Filter")
              inv_scope = filter.get("Prefix") if filter and filter.get("Prefix") else ""
              
              # Does the prefix covered by the inventory report match what the user wants?
              if not scope.startswith(inv_scope):
                bad_scope = True
                continue
              
              dates = list(boto3.resource("s3").Bucket(inv_bucket).objects.filter(Prefix = prefix))
              if len(dates) == 0:
                continue
              
              # Entries are sorted, so the last one is the most recent date.
              result = re.search(r"dt=((\d{4}-\d{2}-\d{2})-(\d{2})-(\d{2}))", dates[-1].key)
              
              if (result):
                ts = datetime.fromisoformat(f"{result.group(2)}T{result.group(3)}:{result.group(4)}:00Z")
                
                if ts >= timestamp:
                
                  response = s3.head_bucket(Bucket = inv_bucket)
                  partition = session.get_partition_for_region(response["BucketRegion"])
                  response_data = {
                    "DateTime" : result.group(1),
                    "InventoryLocation" : f"s3://{inv_bucket}/{prefix[:-3]}",
                    "InventoryArn" : f"arn:{partition}:s3:::{inv_bucket}",
                    "Serde" : serde_map[d["Format"]],
                    "InventorySource" : f"S3 Inventory with id '{id}'"
                  }
                  break
            
            if response_data is None:
              reason = f"Couldn't find an inventory for bucket '{bucket}' in ORC / Parquet format, "\
                          "which includes all object versions, "\
                          f"with fields {fields}, and inventory reports >= {timestamp}."
              
              if (bad_prefix):
                reason += " At least one inventory was ignored because its destination prefix has a leading or trailing '/' character."
              
              if (bad_scope):
                if scope:
                  scope = " the whole bucket."
                else:
                  scope = f" the Prefix '{scope}'."
                
                reason += f" At least one inventory was ignored because its scope didn't include '{scope}."
              
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None, False, reason)
              return
            
            cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)

  InventoryFinderRole:
    Type: "AWS::IAM::Role"
    Condition: CreateInventory
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
              - Effect: Allow
                Action:
                  - "s3:GetInventoryConfiguration"
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3:::${BucketName}"
              - Effect: Allow
                Action:
                  - "s3:ListBucket"
                Resource:
                  - "*"

  InventoryCopier:
    Type: "AWS::Lambda::Function"
    Condition: UseInventoryCSV
    Properties:
      Runtime: python3.13
      MemorySize: 1024
      Timeout: 900
      Handler: "index.handler"
      Role: !GetAtt InventoryCopierRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import re
          import boto3
          import botocore
          from boto3.s3.transfer import TransferConfig
          
          def handler(event, context):
          
              # Only execute on create.
              if event["RequestType"] != "Create":
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  return
                  
              # Where is the CSV source file?
              csv_source = event["ResourceProperties"]["CSVLocation"]
              dest_bucket = event["ResourceProperties"]["CSVDestinationBucket"]
              dest_key = event["ResourceProperties"]["CSVDestinationKey"]
          
              match = re.match("s3://([a-z0-9.-]+)/(.+)", csv_source)
              if match is None:
                  reason = f"Invalid CSV source: {csv_source}"
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)
                  return
              
              source_bucket = match.group(1)
              source_key = match.group(2)
          
              client_config = botocore.config.Config(max_pool_connections = 1000)
              s3 = boto3.client('s3', config = client_config)
          
              config = TransferConfig(max_concurrency = 1000, multipart_chunksize = 250 * 1024 * 1024)           
              copy_source = { 'Bucket' : source_bucket, 'Key' : source_key }
              reason = None
              
              try:
                  # Copy the CSV
                  s3.copy(copy_source, dest_bucket, dest_key, Config = config)
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  return
              
              except Exception as e:
          
                  reason = f"Cannot access {csv_source}"
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)
                  return
            
  InventoryCopierRole:
    Type: "AWS::IAM::Role"
    Condition: UseInventoryCSV
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
              - Effect: Allow
                Action:
                  - "s3:GetObject"
                Resource:
                  - !Join [ "", [ !Sub "arn:${AWS::Partition}:s3:::", !Select [ 1, !Split [ "s3://", !Ref InventoryCSVLocation ] ] ] ]
              - Effect: Allow
                Action:
                  - "s3:PutObject"
                Resource:
                  - !Join [ "", [ !GetAtt AthenaResultsBucket.Arn, "/ImportCSV/inventory.csv" ] ]

  CreateInventoryPITTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: CreateInventory
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Amazon Athena table to hold an Amazon S3 Inventory"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Amazon Athena table to hold an Amazon S3 Inventory

            CREATE EXTERNAL TABLE ${GlueDB}.pit_table(
                     bucket string,
                     key string,
                     version_id string,
                     is_latest boolean,
                     is_delete_marker boolean,
                     size bigint,
                     last_modified_date timestamp,
                     e_tag string,
                     storage_class string,
                     is_multipart_uploaded boolean,
                     replication_status string,
                     encryption_status string,
                     object_lock_retain_until_date bigint,
                     object_lock_mode string,
                     object_lock_legal_hold_status string,
                     intelligent_tiering_access_tier string,
                     bucket_key_status string,
                     checksum_algorithm string,
                     object_access_control_list string,
                     object_owner string
            ) PARTITIONED BY (
                    dt string
            )
            ROW FORMAT SERDE '${Serde}'
              STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'
              OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
              LOCATION '${InventoryLocation}'
              TBLPROPERTIES (
                "projection.enabled" = "true",
                "projection.dt.type" = "date",
                "projection.dt.format" = "yyyy-MM-dd-HH-mm",
                "projection.dt.range" = "2020-01-01-00-00,NOW",
                "projection.dt.interval" = "1",
                "projection.dt.interval.unit" = "HOURS"
              );
        - InventoryLocation: !GetAtt ExecuteInventoryFinder.InventoryLocation
          GlueDB: !Ref GlueDatabase
          Serde: !GetAtt ExecuteInventoryFinder.Serde

  CreateCSVImportTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: UseInventoryCSV
    Properties:
      Database: !Ref GlueDatabase
      Description: "Loads an inventory in CSV format into a temporary table"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Loads an inventory in CSV format into a temporary table

            CREATE EXTERNAL TABLE ${GlueDB}.csv_import(
                     bucket string,
                     key string,
                     version_id string,
                     is_latest boolean,
                     is_delete_marker boolean,
                     size bigint,
                     last_modified_date string,
                     storage_class string,
                     e_tag string
            )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES ("separatorChar" = ",", "quoteChar" = "\"")
            LOCATION '${CSVLocation}'
            TBLPROPERTIES ("skip.header.line.count"="1")

        - CSVLocation: !Sub "s3://${AthenaResultsBucket}/ImportCSV/"
          GlueDB: !Ref GlueDatabase

  CheckCSVImportTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: UseInventoryCSV
    Properties:
      Database: !Ref GlueDatabase
      Description: "Checks to ensure the CSV table only has records from the correct bucket"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Checks to ensure the CSV table only has records from the correct bucket

            SELECT
                     DISTINCT bucket
            FROM
                     ${GlueDB}.csv_import
            WHERE
                     NOT bucket = '${SourceBucket}'

        - SourceBucket: !Ref BucketName
          GlueDB: !Ref GlueDatabase

  CreateCSVPITTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: UseInventoryCSV
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates the initial Point in Time table based on the CSV Import table"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates the initial Point in Time table based on the CSV Import table

            CREATE TABLE ${GlueDB}.pit_table WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/pit_table/',
                write_compression = 'SNAPPY'
            ) AS
            SELECT
                url_decode(key) key,
                version_id,
                is_delete_marker,
                is_latest,
                cast(from_iso8601_timestamp(last_modified_date) as TIMESTAMP) last_modified_date,
                size,
                storage_class,
                cast('' as varchar) intelligent_tiering_access_tier,
                '' dt
            FROM
                ${GlueDB}.csv_import

        - GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket

  CreateMetadataPITTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: CreateInventoryFromMetadata
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates the initial Point in Time table based on the bucket's Metadata table"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates the initial Point in Time table based on the bucket's Metadata table

            CREATE TABLE ${GlueDB}.pit_table WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/pit_table/',
                write_compression = 'SNAPPY'
            ) AS
                    
            (
                WITH my_inventory AS (
                    SELECT 
                        key,
                        version_id,
                        is_delete_marker,
                        CAST(last_modified_date AS timestamp(3)) as last_modified_date,
                        size,
                        storage_class,
                        sequence_number
                    FROM
                        "s3tablescatalog/aws-s3"."${NameSpace}"."${Inventory}" inventory
                    WHERE
                        key LIKE '${Prefix}%'
                ),
                
                -- 
                -- Load journal events that may not yet have been processed into inventory.
                -- For safety, we include events up to 1 hour prior to the start of the last coalesce task.
                --
                my_new_events as (
                    SELECT
                        *
                    FROM (
                        WITH inventory_time_cte AS
                    
                        (
                            SELECT
                                COALESCE(inventory_time_from_property - INTERVAL '1' HOUR, inventory_time_default) AS inventory_time
                            FROM (
                                SELECT
                                    *
                                FROM
                                -- The fallback default includes the entirety of the journal table.
                                (VALUES (TIMESTAMP '2024-12-01 00:00')) AS T (inventory_time_default)
                                LEFT OUTER JOIN
                                -- This side queries the Iceberg table property.
                                (
                                    SELECT
                                        from_unixtime(CAST(value AS BIGINT)/1000) AS inventory_time_from_property 
                                    FROM
                                        "s3tablescatalog/aws-s3"."${NameSpace}"."${Journal}$properties"
                                    WHERE
                                        key = 'aws.s3metadata.oldest-uncoalesced-record-timestamp' 
                                    LIMIT 1
                                )
                                -- Force an unequivocal join.
                                ON TRUE
                            )
                        )
                        
                        -- Find the journal table entries that may not yet have been coalesced
                        SELECT 
                            key,
                            version_id,
                            is_delete_marker,
                            CAST(last_modified_date AS timestamp(3)) as last_modified_date,
                            size,
                            storage_class,
                            CAST(record_timestamp AS timestamp(3)) as record_timestamp,
                            record_type,
                            sequence_number
                        FROM
                            "s3tablescatalog/aws-s3"."${NameSpace}"."${Journal}" journal
                        CROSS JOIN
                            inventory_time_cte t
                        WHERE
                            journal.record_timestamp >= t.inventory_time
                    )
                    WHERE
                        key LIKE '${Prefix}%'
                ),
                
                latest_permanent_deletes AS (
                    -- Find the journal records for permanent deletes, keeping only the highest sequence_number for each key, version_id pair. Bear in mind these DELETE operations can succeed created regardless of the presence of a matching pair in the bucket, and will therefore create a journal entry.
                    SELECT DISTINCT
                        key,
                        version_id,
                        is_delete_marker,
                        FIRST_VALUE(record_timestamp) OVER (PARTITION BY key, version_id ORDER BY sequence_number DESC) as latest_delete_timestamp,
                        MAX(sequence_number) OVER (PARTITION BY key, version_id) as latest_delete_sequence
                    FROM
                        my_new_events
                    WHERE
                        record_type = 'DELETE'
                        AND COALESCE(is_delete_marker, FALSE) = FALSE
                    
                    UNION ALL
                    
                    -- Create dummy permanent delete records for CREATE records with NULL version_id,
                    -- i.e. object overwrites where versioning is not enabled.
                    -- These records will cause existing inventory entries with NULL version_id to be filtered out,
                    -- and allow the newer CREATE record to replace them.
                    SELECT DISTINCT
                        key,
                        version_id,  -- This will be NULL for non-versioned buckets
                        FALSE as is_delete_marker,  -- Dummy deletes are not delete markers
                        CAST(last_modified_date AS timestamp(3)) as latest_delete_timestamp,  -- Use same timestamp as CREATE
                        -- Since sequence_number is varchar, create a sequence that sorts just before the CREATE sequence
                        -- Prepend with a character that sorts early in ASCII (space sorts before most characters)
                        ' ' || sequence_number as latest_delete_sequence
                    FROM
                        my_new_events
                    WHERE
                        record_type = 'CREATE' AND version_id IS NULL
                ),
                
                deduplicated_journal AS (
                    -- Deduplicate journal records, keeping only the highest sequence_number for duplicates, excluding permanent deletes
                    SELECT
                        *
                    FROM (
                        SELECT
                            *,
                            ROW_NUMBER() OVER (
                                PARTITION BY key, version_id, is_delete_marker, size, last_modified_date
                                ORDER BY sequence_number DESC
                            ) as rn
                        FROM
                            my_new_events
                        WHERE
                            last_modified_date IS NOT NULL  -- Only deduplicate non-null records
                            AND is_delete_marker IS NOT NULL
                            AND size IS NOT NULL
                    ) ranked
                    
                    WHERE
                        rn = 1
                    
                    UNION ALL
                    
                    -- Include permanent delete records (these don't participate in deduplication)
                    SELECT
                        *,
                        1 as rn
                    FROM
                        my_new_events
                    WHERE
                        last_modified_date IS NULL
                        OR is_delete_marker IS NULL
                        OR size IS NULL
                ),
                
                journal_filtered AS (
                    -- Filter deduplicated journal records, to new entries for the inventory.
                    -- Permanent deletes (including dummy deletes) are used to filter out journal entries that don't need to be added
                    -- The dummy deletes created above will naturally handle CREATE with NULL version_id replacement logic
                    SELECT 
                        j.key,
                        j.version_id,
                        j.last_modified_date,
                        j.is_delete_marker,
                        j.size,
                        j.storage_class,
                        j.sequence_number,
                        j.record_timestamp,
                        j.record_type,
                        'journal' as source_table
                    FROM
                        deduplicated_journal j
                    LEFT JOIN
                        latest_permanent_deletes lpd 
                        ON
                        j.key = lpd.key
                        AND j.version_id IS NOT DISTINCT FROM lpd.version_id
                    LEFT JOIN my_inventory inv
                        ON j.key = inv.key
                        AND j.version_id IS NOT DISTINCT FROM inv.version_id
                    WHERE 
                        -- Exclude all permanent delete records from output (including dummy deletes)
                        NOT (j.record_type = 'DELETE' AND COALESCE(j.is_delete_marker, FALSE) = FALSE)
                        AND
                        -- Exclude journal records superseded by permanent deletes (including dummy deletes)
                        lpd.key IS NULL
                        AND
                        -- Exclude journal records that already exist in inventory (exact key+version_id match)
                        -- Exception: Include CREATE records with NULL version_id even if they match inventory, because the dummy deletes
                        --  will remove the old inventory entries, allowing replacement by these keyname overwrite operations
                        (inv.key IS NULL OR (j.record_type = 'CREATE' AND j.version_id IS NULL))
                ),
                -- We now have a clean set of new rows to add to our inventory. But first we need to remove deleted rows from our inventory:
                inventory_filtered AS (
                    -- Filter inventory records against latest permanent deletes (including dummy deletes where version_ID is null)
                    SELECT
                        i.*,
                        'inventory' as source_table
                    FROM
                        my_inventory i
                    LEFT JOIN
                        latest_permanent_deletes lpd 
                        ON i.key = lpd.key AND i.version_id IS NOT DISTINCT FROM lpd.version_id
                    WHERE
                        lpd.key IS NULL
                        OR NOT (
                            CASE 
                                WHEN lpd.latest_delete_timestamp > i.last_modified_date + INTERVAL '1' SECOND
                                    THEN TRUE
                                WHEN lpd.latest_delete_timestamp BETWEEN i.last_modified_date - INTERVAL '1' SECOND AND i.last_modified_date + INTERVAL '1' SECOND
                                --use sequence number to decide order, if the permanent delete record_timestamp is close to the inventory row's last_modified_date. 
                                    THEN lpd.latest_delete_sequence > i.sequence_number 
                                ELSE
                                    FALSE
                            END
                        )
                ),
            
                -- Now we can combine the tables:
                coalesced_inventory as (
                    SELECT 
                        key,
                        version_id,
                        is_delete_marker,
                        last_modified_date,
                        size,
                        storage_class,
                        sequence_number
                    FROM
                        inventory_filtered
                    
                    UNION ALL
                    
                    SELECT 
                        key,
                        version_id,
                        is_delete_marker,
                        last_modified_date,
                        size,
                        storage_class,
                        sequence_number
                    FROM
                        journal_filtered
                ),
                
                --
                -- Now we can determine the current version for each key by sorting by sequence number:
                -- Only one record per key will have is_latest = TRUE
                --
                inventory_with_is_latest as (
                    SELECT *,
                        ROW_NUMBER() OVER (
                            PARTITION BY key 
                            ORDER BY sequence_number DESC
                        ) = 1 AS is_latest
                    FROM
                        coalesced_inventory
                )
                --
                -- Output the desired columns 
                --
                SELECT
                    key,
                    version_id,
                    is_delete_marker,
                    is_latest,
                    last_modified_date,
                    size,
                    storage_class,
                    '' as intelligent_tiering_access_tier,
                    '' dt
                FROM inventory_with_is_latest
            )
            
        - GlueDB: !Ref GlueDatabase
          NameSpace: !GetAtt ExecuteMetadataFinder.NameSpace
          Inventory: !GetAtt ExecuteMetadataFinder.Inventory
          Journal: !GetAtt ExecuteMetadataFinder.Journal
          ResultsBucket: !Ref AthenaResultsBucket

  CreateAllVersionsTable:
    Type: "AWS::Athena::NamedQuery"
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Amazon Athena table to hold an Amazon S3 Inventory from a single dt= prefix"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Amazon Athena table to hold an Amazon S3 Inventory from a single dt= prefix.
            -- Takes a single partition, tidies up date, filters by prefix if any
            -- The dt= should be most recent date, so we have visibility into all changes.

            CREATE TABLE ${GlueDB}.all_versions WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/all_versions_table/',
                write_compression = 'SNAPPY'
            ) AS
            SELECT
                key,
                version_id,
                is_delete_marker,
                is_latest,
                last_modified_date,
                size,
                storage_class,
                coalesce(intelligent_tiering_access_tier, '') as intelligent_tiering_access_tier
            FROM
                ${GlueDB}.pit_table
            WHERE
                dt = '${DateTime}' and
                starts_with(key, '${Prefix}')
                
        - DateTime: !If [ CreateInventory, !GetAtt ExecuteInventoryFinder.DateTime, "" ]
          GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket
          Prefix: !Ref Prefix

  CreateAllKeysTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Amazon Athena table to hold the distinct list of keys"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates "All Keys" table, which holds the distinct list of keys.

            CREATE TABLE ${GlueDB}.all_keys WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/all_keys_table/',
                write_compression = 'SNAPPY'
            ) as
            select distinct
                key,
            from
                ${GlueDB}.all_versions
    
        - ResultsBucket: !Ref AthenaResultsBucket
          GlueDB: !Ref GlueDatabase

  CreateLatestVersionsTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Amazon Athena table to hold the latest versions of every file under the prefix"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates "Latest Versions" table, which holds the list of most recent versions.
            -- This will exclude unchanged keys, i.e. ones where the latest version is before the point in time.

            CREATE TABLE ${GlueDB}.latest_versions WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/latest_versions_table/',
                write_compression = 'SNAPPY'
            ) as
            select
                key,
                version_id,
                is_delete_marker,
                size,
                storage_class
            from
                ${GlueDB}.all_versions
            where
                is_latest = true
                and last_modified_date > from_iso8601_timestamp('${TimeStamp}')
    
        - ResultsBucket: !Ref AthenaResultsBucket
          TimeStamp: !Ref TimeStamp
          GlueDB: !Ref GlueDatabase

  CreateDesiredTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollbackOrCopy
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Amazon Athena table to hold the desired version / delete markers of every file under the prefix"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Create "Desired", which holds the list of versions / delete markers immediately before the point in time.

            CREATE TABLE ${GlueDB}.desired WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/desired_table/',
                write_compression = 'SNAPPY'
            ) as
            select
                key,
                max_by(version_id, last_modified_date) as version_id,
                max_by(last_modified_date, last_modified_date) as last_modified_date,
                max_by(is_delete_marker, last_modified_date) as is_delete_marker,
                max_by(is_latest, last_modified_date) as is_latest,
                max_by(size, last_modified_date) as size,
                max_by(storage_class, last_modified_date) as storage_class,
                max_by(intelligent_tiering_access_tier, last_modified_date) as intelligent_tiering_access_tier
            from
                ${GlueDB}.all_versions
            where
                last_modified_date <= from_iso8601_timestamp('${TimeStamp}')
            group by
                key    
                
        - ResultsBucket: !Ref AthenaResultsBucket
          TimeStamp: !Ref TimeStamp
          GlueDB: !Ref GlueDatabase

  CreateDesiredVersionsTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollbackOrCopy
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Amazon Athena table to hold the desired versions of every file under the prefix"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Create "Desired Versions", which holds the list of versions immediately before the point in time.

            CREATE TABLE ${GlueDB}.desired_versions WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/desired_versions_table/',
                write_compression = 'SNAPPY'
            ) as
            select
                *
            from
                ${GlueDB}.desired
            where
                is_delete_marker = false
                
        - ResultsBucket: !Ref AthenaResultsBucket
          TimeStamp: !Ref TimeStamp
          GlueDB: !Ref GlueDatabase

  CreateChangedKeysTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to hold the list of keys that have changed"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to hold the list of keys that have changed
            -- For each Desired Version ID, shows Latest Version ID, delete marker status,
            -- Desired size, Desired storage class (to help determine how best to restore)

            CREATE TABLE ${GlueDB}.changed_keys WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/changed_keys_table/',
                write_compression = 'SNAPPY'
            ) as
            select
                d.key as key,
                d.version_id as desired_version_id,
                l.version_id as latest_version_id,
                d.is_delete_marker as desired_is_delete_marker,
                l.is_delete_marker as latest_is_delete_marker,
                d.size as size,
                d.storage_class as storage_class,
                d.intelligent_tiering_access_tier as intelligent_tiering_access_tier
            from ${GlueDB}.desired_versions d
            inner join ${GlueDB}.latest_versions l on
                d.key = l.key
        
        - ResultsBucket: !Ref AthenaResultsBucket
          GlueDB: !Ref GlueDatabase

  CreateCountVersionsTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to count an object's versions"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to count an object's Versions
            -- For all keys created after the Point In Time, return key and count of non Delete Marker versions.
            
            CREATE TABLE ${GlueDB}.count_versions WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/count_versions_table/',
                write_compression = 'SNAPPY'
            ) as
            select
                l.key as key,
                sum(IF((a.is_delete_marker = true), 0, 1)) count_versions
            from
                ${GlueDB}.latest_versions l
            inner join
                ${GlueDB}.all_versions a on l.key = a.key
            inner join
                ${GlueDB}.desired_versions d on l.key = d.key
            where
                a.last_modified_date > from_iso8601_timestamp('${TimeStamp}')
            group by
                l.key
            
        - ResultsBucket: !Ref AthenaResultsBucket
          TimeStamp: !Ref TimeStamp
          GlueDB: !Ref GlueDatabase

  CreateLatestDMsTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantDMRemoval
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table listing all DMs that are the latest object version"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table listing all DMs that are created after the PIT, with no subsequent object versions
            
            CREATE TABLE ${GlueDB}.latest_dms WITH (
                format = 'ORC',
                external_location = 's3://${ResultsBucket}/Tables/latest_dms_table/',
                write_compression = 'SNAPPY'
            ) as
            select
                av.key as key,
                av.version_id as version_id
            from
                ${GlueDB}.all_versions av
            left join
            (
                -- Find the most recent version of all keys that were created after PIT
                select
                    key,
                    max_by(version_id, last_modified_date) as version_id,
                    max_by(last_modified_date, last_modified_date) as last_modified_date
                from
                    ${GlueDB}.all_versions
                where
                    is_delete_marker = false
                group by
                    key
             ) most_recent
            on 
                av.key = most_recent.key
            where
                av.is_delete_marker
                and av.last_modified_date > greatest(
                    coalesce(most_recent.last_modified_date, from_unixtime(0)),
                    from_iso8601_timestamp('${TimeStamp}')
                )
        
        - ResultsBucket: !Ref AthenaResultsBucket
          TimeStamp: !Ref TimeStamp
          GlueDB: !Ref GlueDatabase

  CreateScenario1Table:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to address Scenario 1"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to address Scenario 1;
            -- Add delete markers where (all version of key have last_modified after Point In Time,
            -- or current version at Point In Time was a delete marker) and current version is not a delete marker

            CREATE TABLE ${GlueDB}.scenario1_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario1_keys_table/'
            ) as
            
            select
                '${Bucket}' as bucket,
                url_encode(s.key) as key
            from (
                
                select -- Keys where all versions are after the Point In Time
                    key
                from
                    ${GlueDB}.all_versions
                group by
                    key
                having
                    min(last_modified_date) > from_iso8601_timestamp('${TimeStamp}')
            
                union
            
                select -- Keys where the desired version is a delete_marker
                    key
                from
                    ${GlueDB}.desired
                where
                    is_delete_marker = true
            ) s
            
            left join
                ${GlueDB}.latest_versions l
            on
                s.key = l.key
            
            where
                l.is_delete_marker <> true -- can be false, or null
 
        - GlueDB: !Ref GlueDatabase
          TimeStamp: !Ref TimeStamp
          ResultsBucket: !Ref AthenaResultsBucket
          Bucket: !Ref BucketName

  CreateScenario2Table:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to address Scenario 2"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to address Scenario 2;
            -- Keys where there are only delete markers (no new objects) after the point in time.

            CREATE TABLE ${GlueDB}.scenario2_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario2_keys_table/'
            ) as
            select
                '${Bucket}' as bucket,
                url_encode(a.key) as key,
                a.version_id as version_id
            from
                ${GlueDB}.count_versions c
            right join
                ${GlueDB}.all_versions a
            on
                c.key = a.key
            -- Only take keys that only have delete markers after the point in time.
            where
                c.count_versions = 0 and
                a.last_modified_date > from_iso8601_timestamp('${TimeStamp}')
            
        - GlueDB: !Ref GlueDatabase
          TimeStamp: !Ref TimeStamp
          ResultsBucket: !Ref AthenaResultsBucket
          Bucket: !Ref BucketName

  CreateScenario2UndoTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to *undo* Scenario 2"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to *undo* Scenario 2;
            -- Replace any delete markers removed when originally running Scenario2
            -- This CSV can then be passed to the Scenario1 Lambda using S3 Batch Operations

            CREATE TABLE ${GlueDB}.scenario2_undo_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario2_undo_keys_table/'
            ) as
            select
                bucket,
                key
            from
                ${GlueDB}.scenario2_keys
            
        - GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket

  CreateScenario3aTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to address Scenario 3a"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to address Scenario 3a;
            -- VersionID from PIT is in GFR/GDA class. Need to restore from async.

            CREATE TABLE ${GlueDB}.scenario3a_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario3a_keys_table/'
            ) as
                select
                    '${Bucket}' as bucket,
                    url_encode(ck.key) as key,
                    ck.desired_version_id as version_id
                from (
                    ${GlueDB}.changed_keys ck
                    inner join ${GlueDB}.count_versions cv ON cv.key = ck.key
                )

            where
                (
                    ck.storage_class in('GLACIER', 'DEEP_ARCHIVE') or
                    ck.intelligent_tiering_access_tier in ('ARCHIVE', 'DEEP_ARCHIVE')
                ) and
                    cv.count_versions > 0

        - GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket
          Bucket: !Ref BucketName

  CreateScenario3bTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to address Scenario 3b"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to address Scenario 3b;
            -- Objects with VersionID newer than at PIT, not covered by Scenario 2, and <= 5GB

            CREATE TABLE ${GlueDB}.scenario3b_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario3b_keys_table/'
            ) as
                select
                    '${Bucket}' as bucket,
                    url_encode(ck.key) as key,
                    ck.desired_version_id as version_id
                from (
                    ${GlueDB}.changed_keys ck
                    inner join ${GlueDB}.count_versions cv ON cv.key = ck.key
                )

            where
                ck.size <= parse_data_size('5GB') and
                ck.storage_class not in('GLACIER', 'DEEP_ARCHIVE') and
                ck.intelligent_tiering_access_tier not in ('ARCHIVE', 'DEEP_ARCHIVE') and
                cv.count_versions > 0

        - GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket
          Bucket: !Ref BucketName

  CreateScenario3cTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantRollback
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to address Scenario 3c"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to address Scenario 3c;
            -- Objects with VersionID newer than at PIT, not covered by Scenario 2, and > 5GB

            CREATE TABLE ${GlueDB}.scenario3c_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario3c_keys_table/'
            ) as
                select
                    '${Bucket}' as bucket,
                    url_encode(ck.key) as key,
                    ck.desired_version_id as version_id
                from (
                    ${GlueDB}.changed_keys ck
                    inner join ${GlueDB}.count_versions cv ON cv.key = ck.key
                )

            where
                ck.size > parse_data_size('5GB') and
                ck.storage_class not in('GLACIER', 'DEEP_ARCHIVE') and
                ck.intelligent_tiering_access_tier not in ('ARCHIVE', 'DEEP_ARCHIVE') and
                cv.count_versions > 0

        - GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket
          Bucket: !Ref BucketName

  CreateScenario4Table:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantDMRemoval
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to address Scenario 4"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to address Scenario 4;
            -- Remove delete markers that are the latest version of an object.

            CREATE TABLE ${GlueDB}.scenario4_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario4_keys_table/'
            ) as
                select
                    '${Bucket}' as bucket,
                    url_encode(key) as key,
                    version_id
                from (
                    ${GlueDB}.latest_dms
                )

        - GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket
          Bucket: !Ref BucketName

  CreateScenario4UndoTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantDMRemoval
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to *undo* Scenario 4"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to *undo* Scenario 4;
            -- Replace any delete markers removed when originally running Scenario4
            -- This CSV can then be passed to the Scenario1 Lambda using S3 Batch Operations

            CREATE TABLE ${GlueDB}.scenario4_undo_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario4_undo_keys_table/'
            ) as
            select
                bucket,
                key
            from
                ${GlueDB}.scenario4_keys
            
        - GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket

  CreateScenario5aTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantCopy
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to address Scenario 5a"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to address Scenario 5a;
            -- VersionID from PIT is in GFR/GDA class. Need to restore from async.

            CREATE TABLE ${GlueDB}.scenario5a_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario5a_keys_table/'
            ) as
                select
                    '${Bucket}' as bucket,
                    url_encode(key) as key,
                    version_id
                from
                    ${GlueDB}.desired_versions
                
                where
                    storage_class in('GLACIER', 'DEEP_ARCHIVE') or
                    intelligent_tiering_access_tier in ('ARCHIVE', 'DEEP_ARCHIVE')

        - GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket
          Bucket: !Ref BucketName

  CreateScenario5bTable:
    Type: "AWS::Athena::NamedQuery"
    Condition: WantCopy
    Properties:
      Database: !Ref GlueDatabase
      Description: "Creates an Athena table to address Scenario 5b"
      WorkGroup: !Ref AthenaWorkGroup
      QueryString: !Sub
        - |
            -- Creates an Athena table to address Scenario 5b;
            -- Objects not in archive.

            CREATE TABLE ${GlueDB}.scenario5b_keys WITH (
                format = 'TEXTFILE',
                field_delimiter = ',',
                write_compression = 'NONE',
                external_location = 's3://${ResultsBucket}/Tables/scenario5b_keys_table/'
            ) as
                select
                    '${Bucket}' as bucket,
                    url_encode(key) as key,
                    version_id
                from
                    ${GlueDB}.desired_versions

                where
                    storage_class not in('GLACIER', 'DEEP_ARCHIVE') and
                    intelligent_tiering_access_tier not in ('ARCHIVE', 'DEEP_ARCHIVE')

        - GlueDB: !Ref GlueDatabase
          ResultsBucket: !Ref AthenaResultsBucket
          Bucket: !Ref BucketName

  ManifestMaker:
    Type: "AWS::Lambda::Function"
    Properties:
      Runtime: python3.13
      MemorySize: 1024
      Timeout: 900
      Handler: "index.handler"
      Role: !GetAtt ManifestMakerRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import concurrent.futures
          import time
          import json
          import uuid
          import boto3
          import boto3.session
          import botocore
          import botocore.config
          
          # Takes a list of S3 keys and combines them into files of >= 5MB. Deletes the old keys.
          def combine(s3, bucket, prefix, keys):
          
              # Special case. Only received 1 file? Just return it unchanged.
              if len(keys) == 1:
                  return keys
              
              buffer = bytearray()
              key_count = 0
              new_names = []
              
              for k in keys:
                  
                  key_count += 1
                  response = s3.get_object(Bucket = bucket, Key = k)
                  buffer.extend(response["Body"].read())
                  
                  # Have we collected more than 5MB?
                  if len(buffer) >= 5*1024*1024:
                      new_names += [ write_object(s3, bucket, prefix, buffer) ]
                      buffer = bytearray() # Empty the buffer
                      key_count = 0
                  
              # One file left, just return it intact.
              if key_count == 1:
                  new_names += [ k ]
              
              # Multiple files left in buffer. Combine them.
              elif key_count > 1:
                  new_names += [ write_object(s3, bucket, prefix, buffer) ]
              
              return new_names
          
          # Writes an object with a new, random name.
          def write_object(s3, bucket, prefix, bytes):
            
              name = f"{prefix}{uuid.uuid4()}-{time.time()}.csv" # Random name.
              response = s3.put_object(Bucket = bucket, Key = name, Body = bytes)
              
              return name
          
          def handler(event, context):
            
              # Only execute on create.
              if event["RequestType"] != "Create":
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  return
              
              bucket = event["ResourceProperties"]["SourceBucket"]
              table_src = event["ResourceProperties"]["TableSource"]
              manifest_dst = event["ResourceProperties"]["ManifestDestination"]
              
              session = boto3.session.Session()
              s3 = session.client("s3")
              
              for t in event["ResourceProperties"]["Tables"]:
              
                  dst_key = f"{manifest_dst}{t}.csv"
                  prefix = f"{table_src}{t}_keys_table/"
                  src_keys = []
                  combine_keys = []
                  found = False
                  
                  for p in s3.get_paginator("list_objects_v2").paginate(Bucket = bucket, Prefix = prefix):
                      
                      for c in p.get("Contents", []):
                          
                          found = True
                          if c["Size"] < 5*1024*1024:
                              combine_keys += [ c["Key"] ]
                          else:
                              src_keys += [ c["Key"] ]
                      
                  if found:
                      
                      try:
                          
                          # Combine any parts < 5MB into objects of at least 5MB.    
                          src_keys += combine(s3, bucket, "tmp/", combine_keys)
                          
                          # Create a new S3 client, but this time with enough pooled connections
                          # to upload all parts in parallel.
                          config = botocore.config.Config(max_pool_connections = int(len(src_keys)))
                          s3 = session.client("s3", config = config)
                          
                          response = s3.create_multipart_upload(
                              Bucket = bucket,
                              Key = dst_key
                          )
                          
                          upload_id = response["UploadId"]
                          
                          # Create a separate thread for each part being uploaded.
                          part = 1
                          futures = []
                          executor = concurrent.futures.ThreadPoolExecutor(max_workers = len(src_keys))
                          for key in src_keys:
                              futures += [ executor.submit(upload_part, s3, bucket, dst_key, { "Bucket" : bucket, "Key" : key } , part, upload_id) ]
                              part += 1
                          
                          parts = []
                          for r in concurrent.futures.as_completed(futures):
                              parts += [ r.result() ]
                          
                          executor.shutdown()
                          
                          parts.sort(key = lambda e: e["PartNumber"])
                          
                          response = s3.complete_multipart_upload(
                              Bucket = bucket,
                              Key = dst_key,
                              MultipartUpload = { "Parts" : parts },
                              UploadId = upload_id
                          )
                          
                          waiter = s3.get_waiter('object_exists')
                          waiter.wait(
                              Bucket = bucket,
                              Key = dst_key,
                              WaiterConfig = { 'Delay' : 10, 'MaxAttempts' : 60 }
                          )
                      
                      except botocore.exceptions.ClientError as e:
                          reason = f'S3 processing failed for table {t} with {e.response["Error"]["Code"]}: {e.response["Error"]["Message"]}'
                          cfnresponse.send(event, context, cfnresponse.FAILED, {}, None, False, reason)
                      
                      except botocore.exceptions.WaiterError as e:
                          reason = f'Generating manifest for table {t} still not complete.'
                          cfnresponse.send(event, context, cfnresponse.FAILED, {}, None, False, reason)
                  
                  else:
                      print(f"No source files found for table {t}.")
                  
              response_data = { "Manifests" : f"s3://{bucket}/{manifest_dst}" }
              cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
          
          def upload_part(s3, bucket, key, src, part, upload_id):
            
            response = s3.upload_part_copy(
                Bucket = bucket,
                Key = key,
                CopySource = src,
                PartNumber = part,
                UploadId = upload_id
            )
            
            return { "ETag" : response["CopyPartResult"]["ETag"], "PartNumber" : part }
  
  ManifestMakerRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
              - Effect: Allow
                Action:
                  - "s3:ListBucket"
                  - "s3:GetObject"
                  - "s3:PutObject"
                Resource:
                  - !GetAtt AthenaResultsBucket.Arn
                  - !Sub "${AthenaResultsBucket.Arn}/*"
                  
  QueryExecutor:
    Type: "AWS::Lambda::Function"
    Properties:
      Runtime: python3.13
      MemorySize: 1024
      Timeout: 900
      Handler: "index.handler"
      Role: !GetAtt QueryExecutorRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import json
          import time
          import boto3
          import re
          
          def handler(event, context):
          
              # Only execute on create.
              if event["RequestType"] != "Create":
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  return
              
              # Some queries should / should not be executed if we've found a Metadata configuration.
              # No easy way to do this natively in CloudFormation, so we have to hard-code checks here.
              if ( (event["ResourceProperties"].get("SkipIfMetadataFound") and event["ResourceProperties"].get("NameSpace") != "default") or
                   (event["ResourceProperties"].get("OnlyIfMetadataFound") and event["ResourceProperties"].get("NameSpace") == "default") ):
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {"State" : "SUCCEEDED"})
                  return
                  
              query_id = event["ResourceProperties"]["QueryID"]
              
              # Where to place the results
              results_location = event["ResourceProperties"]["ResultsLocation"]
              
              workgroup = event["ResourceProperties"]["WorkGroup"]
              
              s3 = boto3.client("s3")
              athena = boto3.client("athena")
              
              query = athena.get_named_query(NamedQueryId = query_id)
              
              try:
                response = athena.start_query_execution(
                    QueryString = query["NamedQuery"]["QueryString"],
                    ResultConfiguration = { "OutputLocation" : results_location },
                    WorkGroup = workgroup
                )
              
                execution_id = response["QueryExecutionId"]
                response = athena.get_query_execution(QueryExecutionId = execution_id)
              
                # Wait for query to finish.
                while response["QueryExecution"]["Status"]["State"] in ["QUEUED", "RUNNING"]:
                  
                    time.sleep(5)
                    response = athena.get_query_execution(QueryExecutionId = execution_id)
              
                (bucket, prefix) = results_location[5:].split("/", 1)
              
                # Remove metadata files so we just have CSVs to import.
                print(f"Paginating s3://{bucket}/{prefix}")
                for p in s3.get_paginator("list_objects_v2").paginate(Bucket = bucket, Prefix = prefix):
                    for c in p.get("Contents", []):
                      
                        if c["Key"].endswith(".metadata"):
                            r = s3.delete_object(Bucket = bucket, Key = c["Key"])
              
                if response["QueryExecution"]["Status"]["State"] == "SUCCEEDED":
                  
                    cfnresponse.send(event, context, cfnresponse.SUCCESS, {"State" : "SUCCEEDED"})
              
                else:
                    reason = str(response["QueryExecution"]["Status"]["AthenaError"])
                    cfnresponse.send(event, context, cfnresponse.FAILED, {}, None, False, reason)
              
              except Exception as e:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, None, False, f"Caught exception: {e}")
          
  QueryExecutorRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
              - Effect: Allow
                Action:
                  - "athena:GetNamedQuery"
                  - "athena:StartQueryExecution"
                  - "athena:GetQueryExecution"
                  - "athena:GetQueryResults"
                Resource:
                  - !Sub "arn:${AWS::Partition}:athena:${AWS::Region}:${AWS::AccountId}:workgroup/${AthenaWorkGroup}"
              - Effect: Allow
                Action:
                  - "glue:CreateDatabase"
                  - "glue:CreateTable"
                Resource:
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/*"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/*"
              - Effect: Allow
                Action:
                  - "glue:Get*"
                Resource:
                  - "*"
              - Effect: Allow
                Action:
                  - "lakeformation:GetDataAccess"
                Resource:
                  - "*"
              - Effect: Allow
                Action:
                  - "s3:GetObject"
                Resource:
                  - !Sub "${AthenaResultsBucket.Arn}/*"
                  - !If [ CreateInventory, !Sub "${ExecuteInventoryFinder.InventoryArn}/*", !Ref "AWS::NoValue" ]
                  - !If [ UseInventoryCSV, !Join [ "", [ !Sub "arn:${AWS::Partition}:s3:::", !Select [ 1, !Split [ "s3://", !Ref InventoryCSVLocation ] ], "*" ] ], !Ref "AWS::NoValue" ]
              - Effect: Allow
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !GetAtt AthenaResultsBucket.Arn
                  - !If [ CreateInventory, !GetAtt ExecuteInventoryFinder.InventoryArn, !Ref "AWS::NoValue" ]
                  - !If [ UseInventoryCSV, !Join [ "", [ !Sub "arn:${AWS::Partition}:s3:::", !Select [ 2, !Split [ "/", !Ref InventoryCSVLocation ] ] ] ], !Ref "AWS::NoValue" ]
              - Effect: Allow
                Action:
                  - "s3:PutObject"
                  - "s3:DeleteObject"
                Resource:
                  - !Sub "${AthenaResultsBucket.Arn}/*"
              - Effect: Allow
                Action:
                  - "s3:GetBucketLocation"
                Resource:
                  - !GetAtt AthenaResultsBucket.Arn
        
  CSVChecker:
    Condition: UseInventoryCSV
    Type: "AWS::Lambda::Function"
    Properties:
      Runtime: python3.13
      MemorySize: 1024
      Timeout: 60
      Handler: "index.handler"
      Role: !GetAtt QueryExecutorRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import time
          import boto3
          
          def handler(event, context):
          
              # Only execute on create.
              if event["RequestType"] != "Create":
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  return
                  
              query_id = event["ResourceProperties"]["QueryID"]
              
              # Where to place the results
              results_location = event["ResourceProperties"]["ResultsLocation"]
              
              # What CSV file did we read?
              csv_location = event["ResourceProperties"]["CSVLocation"]
          
              # What bucket should we expect to find?
              bucket = event["ResourceProperties"]["Bucket"]
          
              workgroup = event["ResourceProperties"]["WorkGroup"]
              
              s3 = boto3.client("s3")
              athena = boto3.client("athena")
              
              query = athena.get_named_query(NamedQueryId = query_id)
              
              try:
                response = athena.start_query_execution(
                    QueryString = query["NamedQuery"]["QueryString"],
                    ResultConfiguration = { "OutputLocation" : results_location },
                    WorkGroup = workgroup
                )
              
                execution_id = response["QueryExecutionId"]
                response = athena.get_query_execution(QueryExecutionId = execution_id)
              
                # Wait for query to finish.
                while response["QueryExecution"]["Status"]["State"] in ["QUEUED", "RUNNING"]:
                  
                    time.sleep(5)
                    response = athena.get_query_execution(QueryExecutionId = execution_id)
              
                if response["QueryExecution"]["Status"]["State"] == "SUCCEEDED":
                  
                  # Did the query find buckets in the CSV that don't match the target bucket name?
                  response = athena.get_query_results(QueryExecutionId = execution_id)
                  bad_buckets = list(map(lambda r : f"'{r["Data"][0]["VarCharValue"]}'", response["ResultSet"]["Rows"]))[1:]
          
                  if bad_buckets:
                      reason = f"CSV inventory files in '{csv_location}' contain bucket names that don't match target bucket name: '{bucket}'. Found {', '.join(bad_buckets)}"
                      cfnresponse.send(event, context, cfnresponse.FAILED, {}, None, False, reason)
                      return
          
                  else:
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {"State" : "SUCCEEDED"})
                      return
              
                else:
                    reason = str(response["QueryExecution"]["Status"]["AthenaError"])
                    cfnresponse.send(event, context, cfnresponse.FAILED, {}, None, False, reason)
                    return
              
              except Exception as e:
                cfnresponse.send(event, context, cfnresponse.FAILED, {}, None, False, f"Caught exception: {e}")
                return
                
  S3BatchOpsCreator:
    Type: "AWS::Lambda::Function"
    Properties:
      Runtime: python3.13
      MemorySize: 1024
      Timeout: 60
      Handler: "index.handler"
      Role: !GetAtt S3BatchOpsCreatorRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import boto3
          import json
          import botocore
          import os
          
          def handler(event, context):
          
              #
              # Delete any existing S3 Batch Operations jobs on stack delete.
              #
              if event["RequestType"] == "Delete":
                  
                  job_id = event['PhysicalResourceId']
                  if job_id == "NO_JOB":
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      return
                  
                  try:
                      sts = boto3.client("sts")
                      account_id = sts.get_caller_identity()["Account"]
                      
                      s3control = boto3.client("s3control")
                      response = s3control.update_job_status(
                          AccountId = account_id,
                          JobId = job_id,
                          RequestedJobStatus= "Cancelled",
                          StatusUpdateReason = f"Cancelling job due to deletion of CloudFormation Stack '{event['StackId']}'"
                      )
                  
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      return
              
                  except botocore.exceptions.ClientError as e:
                  
                      print(f"Warning: couldn't delete job '{event['PhysicalResourceId']}': {e}")
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      return
              
              #
              # Update. Do nothing.
              #
              if event["RequestType"] == "Update":
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  return
              
              #
              # Create job.
              #
              # Location of the manifest file.
              manifest_bucket = event["ResourceProperties"]["ManifestBucket"]
              manifest_key = event["ResourceProperties"]["ManifestKey"]
              manifest_arn = f"arn:aws:s3:::{manifest_bucket}/{manifest_key}"
              
              # Manifest fields.
              fields = ["Bucket", "Key"]
              if event["ResourceProperties"].get("HasVersionIds"):
                  fields.append("VersionId")
              
              # Where to put report
              report_bucket = event["ResourceProperties"]["ReportBucket"]
              report_prefix = event["ResourceProperties"]["ReportPrefix"]
              
              # Description of job
              description = event["ResourceProperties"]["Description"]
              
              # Location where files should be copied to
              destination_bucket = event["ResourceProperties"].get("DestinationBucket")
              
              # Role for S3 Batch Operations to use
              role_arn = event["ResourceProperties"]["S3BatchOpsRoleArn"]
              
              # Generate unique client request token, max 64 characters.
              token = event["StackId"].split("/")[2] + event["RequestId"]
              token = token.replace("-", "")[0:64]
              
              # What storage class to use for the copy
              storage_class = event["ResourceProperties"].get("StorageClass", "")
              
              # What function to use for Lambda copy jobs
              function_arn = event["ResourceProperties"].get("FunctionArn")
              
              # What KMS key to use for encryption when copying objects
              kms_key = event["ResourceProperties"].get("KMSKey")
              
              # Should we start the job automatically?
              start_job = event["ResourceProperties"].get("StartJob", "NO") == "YES"
              
              try:
                  sts = boto3.client("sts")
                  account_id = sts.get_caller_identity()["Account"]
          
                  # Find the eTag for the manifest
                  s3 = boto3.client("s3")
                  response = s3.head_object(
                      Bucket = manifest_bucket,
                      Key = manifest_key
                  )
                  
                  # Remove open and closing quote marks on eTag
                  etag = response["ETag"][1:-1]
              
                  # Create the S3 Batch Operations job
                  s3control = boto3.client("s3control")
                  
                  kwargs = {
                      "AccountId" : account_id,
                      "ConfirmationRequired" : not start_job,
                      "Report" : {
                          "Bucket": report_bucket,
                          "Prefix": report_prefix,
                          "Format": "Report_CSV_20180820",
                          "Enabled": True,
                          "ReportScope": "AllTasks"
                      },
                      "ClientRequestToken" : token,
                      "Manifest" : {
                          "Location": {
                              "ObjectArn": manifest_arn,
                              "ETag": etag
                          },
                          "Spec": {
                              "Format": "S3BatchOperations_CSV_20180820",
                              "Fields": fields
                          }
                      },
                      "Priority" : 1,
                      "RoleArn" : role_arn,
                      "Description" : f"S3 Point in Time Rollback - {description} - for CloudFormation Stack '{event['StackId']}'"
                  }
                  
                  if function_arn: # A Lambda job (either Add / Remove DMs, or Scenario 5b Copy job)
                      
                      kwargs["Operation"] = {
                          "LambdaInvoke": {
                              "FunctionArn": function_arn,
                              "InvocationSchemaVersion" : "2.0"
                          }
                      }
                      
                      user_args = {}
                      
                      if destination_bucket: # A Scenario 5b copy job
                          user_args["DestinationBucket"] = destination_bucket
                      
                      if storage_class: # A Scenario 5b copy with storage class specified
                          user_args["StorageClass"] = storage_class
                      
                      if kms_key: # A Scenario 5b copy with KMS encryption
                          user_args["KMSKey"] = kms_key
                          
                      if user_args:
                          kwargs["Operation"]["LambdaInvoke"]["UserArguments"] = user_args
                
                  else: # A Scenario3b copy job
                      kwargs["Operation"] = {
                          "S3PutObjectCopy": {
                              "TargetResource": f"arn:aws:s3:::{destination_bucket}",
                              "MetadataDirective": "COPY",
                              "StorageClass": storage_class,
                              "CannedAccessControlList": "bucket-owner-full-control"
                          }
                      }
                      
                      if kms_key: # Want KMS encryption
                          kwargs["Operation"]["S3PutObjectCopy"]["SSEAwsKmsKeyId"] = kms_key
                          kwargs["Operation"]["S3PutObjectCopy"]["NewObjectMetadata"] = { "SSEAlgorithm" : "KMS" }
                      
                  response = s3control.create_job(**kwargs)
                  
                  url = f"https://{os.environ['AWS_REGION']}.console.aws.amazon.com/s3/jobs/{response['JobId']}"
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, { "Output": url}, response['JobId'])
              
              except botocore.exceptions.ClientError as e:
                  
                  if e.response["ResponseMetadata"]["HTTPStatusCode"] == 404:
                      output = "*** Manifest does not exist. Skipping batch job creation. ***"
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Output" : output}, "NO_JOB")
                      return
              
                  reason = f"Failed to create S3 Batch Operations Job: {e}"
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, None , False, reason)                   

  S3BatchOpsCreatorRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
              - Effect: Allow
                Action:
                  - "s3:GetObject"
                Resource:
                  - !Sub "${AthenaResultsBucket.Arn}/Manifests/*"
              - Effect: Allow
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !GetAtt AthenaResultsBucket.Arn
                Condition:
                  StringLike:
                    "s3:prefix": "Manifests/*"
              - Effect: Allow
                Action:
                  - "s3:CreateJob"
                  - "s3:UpdateJobStatus"
                Resource:
                  - "*"
              - Effect: Allow
                Action:
                  - "iam:PassRole"
                Resource:
                  - !GetAtt S3BatchOpsExecutorRole.Arn
        
  S3BatchOpsExecutorRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - batchoperations.s3.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "s3:GetObject*"
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3:::${BucketName}/${Prefix}*"
                  - !Sub "${AthenaResultsBucket.Arn}/Manifests/*"
              - Effect: Allow
                Action:
                  - "s3:PutObject*"
                Resource:
                  - !Join [ "", [ !Sub "arn:${AWS::Partition}:s3:::", !If [ WantCopy, !Ref DestinationBucket, !Ref BucketName ],  !Sub "/${Prefix}*" ] ]
                  - !Sub "${AthenaResultsBucket.Arn}/Reports/*"
              - Effect: Allow
                Action:
                  - "lambda:InvokeFunction"
                Resource:
                  - !If [ WantRollbackOrDMRemoval, !GetAtt S3BatchOpsDeleteFunction.Arn, !Ref "AWS::NoValue" ]
                  - !If [ WantRollbackOrCopy, !GetAtt S3BatchOpsCopyFunction.Arn, !Ref "AWS::NoValue" ]

  S3BatchOpsDeleteFunction:
    Condition: WantRollbackOrDMRemoval
    Type: "AWS::Lambda::Function"
    Properties:
      Runtime: python3.13
      MemorySize: 256
      Timeout: 900
      Handler: "index.handler"
      Role: !GetAtt S3BatchOpsDeleteFunctionRole.Arn
      Code:
        ZipFile: |
          import boto3
          from urllib.parse import unquote_plus
          from botocore.exceptions import ClientError
          
          s3 = boto3.client('s3')
          
          def handler(event, context):
          
              results = {
                  'invocationSchemaVersion': event['invocationSchemaVersion'],
                  'treatMissingKeysAs': 'PermanentFailure',
                  'invocationId': event['invocationId'],
                  'results': []
              }
              
              for task in event['tasks']:
          
                  task_id = task['taskId']
                  s3_bucket = task['s3Bucket']
                  s3_key = unquote_plus(task['s3Key'])
                  s3_version_id = task.get('s3VersionId', None)
          
                  result = {'taskId' : task_id }
          
                  try:           
                      # Delete the object
                      if s3_version_id:
                        s3.delete_object(Bucket=s3_bucket, Key=s3_key, VersionId=s3_version_id)
                      else:
                        s3.delete_object(Bucket=s3_bucket, Key=s3_key, IfMatch='*')                      
          
                      result['resultCode'] = "Succeeded"
                      result['resultString'] = "Deleted"
                  
                  except ClientError as e:
                      # Handle specific S3 errors
                      error_code = e.response['Error']['Code']
                      error_message = e.response['Error']['Message']
                      request_id = e.response['ResponseMetadata']['RequestId']
                      http_status_code = e.response['ResponseMetadata']['HTTPStatusCode']
                      retry_attempts = e.response['ResponseMetadata']['RetryAttempts']
                  
                      if error_code == 'NoSuchKey':
                          # Object doesn't exist - might have been deleted already
                          result['resultCode'] = "Succeeded"
                          result['resultString'] = "Not found."
                          
                      elif error_code == 'AccessDenied':
                          # Permission issues
                          result['resultCode'] = "PermanentFailure"
                          result['resultString'] = f"Access denied. RequestID = {request_id}."
          
                      else:
                          # Other client errors
                          result['resultCode'] = "PermanentFailure"
                          result['resultString'] = f"{error_code} - {error_message}. HTTP Status Code = {http_status_code}. Retry Attempts = {retry_attempts}. RequestID = {request_id}."
          
                  results['results'].append(result)
              return results

  S3BatchOpsDeleteFunctionRole:
    Condition: WantRollbackOrDMRemoval
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "s3:DeleteObject*"
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3:::${BucketName}/${Prefix}*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"

  S3BatchOpsCopyFunction:
    Type: "AWS::Lambda::Function"
    Condition: WantRollbackOrCopy
    Properties:
      Runtime: python3.13
      MemorySize: 2048
      Timeout: 900
      Handler: "index.handler"
      Role: !GetAtt S3BatchOpsCopyFunctionRole.Arn
      Code:
        ZipFile: |
          import boto3
          import botocore
          from urllib.parse import unquote_plus
          from botocore.exceptions import ClientError
          from boto3.s3.transfer import TransferConfig
          
          client_config = botocore.config.Config(max_pool_connections = 1000)
          s3 = boto3.client('s3', config = client_config)
          
          def handler(event, context):
          
              results = {
                  'invocationSchemaVersion': event['invocationSchemaVersion'],
                  'treatMissingKeysAs': 'PermanentFailure',
                  'invocationId': event['invocationId'],
                  'results': []
              }
              
              for task in event['tasks']:
          
                  task_id = task['taskId']
                  s3_source_bucket = task['s3Bucket']
                  s3_key = unquote_plus(task['s3Key'])
                  s3_version_id = task['s3VersionId']
                  s3_destination_bucket = event['job']['userArguments']['DestinationBucket']
                  storage_class = event['job']['userArguments']['StorageClass']
                  kms_key = event['job']['userArguments'].get("KMSKey")
          
                  result = {'taskId' : task_id }
          
                  try:
                  
                      # Get Metadata and Tags
                      response = s3.head_object(Bucket = s3_source_bucket, Key = s3_key, VersionId = s3_version_id)
                      metadata = response['Metadata']
                      
                      response = s3.get_object_tagging(Bucket = s3_source_bucket, Key = s3_key, VersionId = s3_version_id)
                      tag_set = response['TagSet']
                      
                      # Up to 1,000 threads for transferring, each part size max 250MB.
                      config = TransferConfig(max_concurrency = 1000, multipart_chunksize = 250 * 1024 * 1024)
                      
                      copy_source = { 'Bucket' : s3_source_bucket, 'Key' : s3_key, 'VersionId' : s3_version_id }
                      extra_args = {
                          "StorageClass" : storage_class,
                          "Metadata" : metadata,
                          "MetadataDirective" : "REPLACE"
                      }
                      
                      if kms_key:
                          extra_args["SSEKMSKeyId"] = kms_key
                          extra_args["ServerSideEncryption"] = "aws:kms"
                      
                      # Copy the object                      
                      s3.copy(copy_source, s3_destination_bucket, s3_key, ExtraArgs = extra_args, Config = config)
                      
                      # Set tags                      
                      s3.put_object_tagging(Bucket = s3_destination_bucket, Key = s3_key, Tagging = { 'TagSet' : tag_set })
          
                      result['resultCode'] = "Succeeded"
                      result['resultString'] = "Copied"
                  
                  except ClientError as e:
                      # Handle errors
                      error_code = e.response['Error']['Code']
                      error_message = e.response['Error']['Message']
                  
                      result['resultCode'] = "PermanentFailure"
                      result['resultString'] = f"Error copying {s3_key} with version {s3_version_id} from bucket {s3_source_bucket} to bucket {s3_destination_bucket}: {error_code} - {error_message}"
          
                  results['results'].append(result)
          
              return results

  S3BatchOpsCopyFunctionRole:
    Type: "AWS::IAM::Role"
    Condition: WantRollbackOrCopy
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "s3:GetObject*"
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3:::${BucketName}/${Prefix}*"
              - Effect: Allow
                Action:
                  - "s3:PutObject*"
                Resource:
                  - !If [ WantCopy, !Sub "arn:${AWS::Partition}:s3:::${DestinationBucket}/${Prefix}*", !Sub "arn:${AWS::Partition}:s3:::${BucketName}/${Prefix}*" ]
              - Effect: Allow
                Action:
                  - "logs:CreateLogGroup"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource:
                  - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
  
  ExecuteCheckVersioning:
    Type: "Custom::ExecuteCheckVersioningFunction"
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt CheckVersioning.Arn
      SourceBucket: !Ref BucketName
      DestinationBucket: !If [ WantCopy, !Ref DestinationBucket, !Ref "AWS::NoValue" ]

  ExecuteClean:
    Type: "Custom::ExecuteCleanerFunction"
    DependsOn:
    - ExecuteCheckVersioning
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt BucketCleaner.Arn
      Bucket: !Ref AthenaResultsBucket

  ExecuteMetadataFinder:
    Type: "Custom::ExecuteMetadataFinderFunction"
    Condition: CreateInventoryFromMetadata
    DependsOn:
    - ExecuteClean
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt MetadataFinder.Arn
      Bucket: !Ref BucketName
      UseMetadata: !Ref UseMetadata
     
  ExecuteInventoryFinder:
    Type: "Custom::ExecuteInventoryFinderFunction"
    Condition: CreateInventory
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt InventoryFinder.Arn
      Bucket: !Ref BucketName
      Prefix: !Ref Prefix
      TimeStamp: !Ref TimeStamp
      NameSpace: !If [ CreateInventoryFromMetadata, !GetAtt ExecuteMetadataFinder.NameSpace, "default" ]
      ResultsBucketArn: !GetAtt AthenaResultsBucket.Arn
      
  ExecuteCreateInventoryPITTable:
    Type: "Custom::ExecuteCreateInventoryPITTableFunction"
    Condition: CreateInventory
    DependsOn:
    - ExecuteClean
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateInventoryPITTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateInventoryPITTable/"
      SkipIfMetadataFound: true
      NameSpace: !If [ CreateInventoryFromMetadata, !GetAtt ExecuteMetadataFinder.NameSpace, "default" ]

  ExecuteCreateMetadataPITTable:
    Type: "Custom::ExecuteCreateMetadataPITTableFunction"
    Condition: CreateInventoryFromMetadata
    DependsOn:
    - ExecuteClean
    - GluePermissions
    - S3TablesPermissions
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateMetadataPITTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateMetadataPITTable/"
      OnlyIfMetadataFound: true
      NameSpace: !GetAtt ExecuteMetadataFinder.NameSpace

  ExecuteInventoryCopier:
    Type: "Custom::ExecuteCreateCSVImportTableFunction"
    Condition: UseInventoryCSV
    DependsOn:
    - ExecuteClean
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt InventoryCopier.Arn
      CSVLocation: !Ref InventoryCSVLocation 
      CSVDestinationBucket: !Ref AthenaResultsBucket
      CSVDestinationKey: "ImportCSV/inventory.csv"      

  ExecuteCreateCSVImportTable:
    Type: "Custom::ExecuteCreateCSVImportTableFunction"
    Condition: UseInventoryCSV
    DependsOn:
    - ExecuteInventoryCopier
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateCSVImportTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateCSVImportTable/"

  ExecuteCSVChecker:
    Type: "Custom::ExecuteCSVCheckFunction"
    Condition: UseInventoryCSV
    DependsOn:
    - ExecuteCreateCSVImportTable
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt CSVChecker.Arn
      QueryID: !GetAtt CheckCSVImportTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CheckCSV/"
      CSVLocation: !Ref InventoryCSVLocation
      Bucket: !Ref BucketName

  ExecuteCreateCSVPITTable:
    Type: "Custom::ExecuteCreateCSVPITTableFunction"
    Condition: UseInventoryCSV
    DependsOn:
    - ExecuteCSVChecker
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateCSVPITTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateCSVPITTable/"

  ExecuteCreateAllVersionsTable:
    Type: "Custom::ExecuteCreateAllVersionsTableFunction"
    DependsOn:
    - ExecuteClean
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateAllVersionsTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateAllVersionsTable/"
      Dependency:
        - !If [ CreateInventory, !GetAtt ExecuteCreateInventoryPITTable.State, !GetAtt ExecuteCreateCSVPITTable.State ]
        - !If [ CreateInventoryFromMetadata, !GetAtt ExecuteCreateMetadataPITTable.State, !Ref "AWS::NoValue" ]

  ExecuteCreateLatestVersionsTable:
    Type: "Custom::ExecuteCreateLatestVersionsTableFunction"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteCreateAllVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateLatestVersionsTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateLatestVersionsTable/"

  ExecuteCreateDesiredTable:
    Type: "Custom::ExecuteCreateDesiredTableFunction"
    Condition: WantRollbackOrCopy
    DependsOn:
    - ExecuteClean
    - ExecuteCreateAllVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateDesiredTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateDesiredTable/"

  ExecuteCreateDesiredVersionsTable:
    Type: "Custom::ExecuteCreateDesiredVersionsTableFunction"
    Condition: WantRollbackOrCopy
    DependsOn:
    - ExecuteClean
    - ExecuteCreateDesiredTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateDesiredVersionsTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateDesiredVersionsTable/"

  ExecuteCreateChangedKeysTable:
    Type: "Custom::ExecuteCreateChangedKeysTableFunction"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteCreateDesiredVersionsTable
    - ExecuteCreateLatestVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateChangedKeysTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateChangedKeysTable/"

  ExecuteCreateCountVersionsTable:
    Type: "Custom::ExecuteCreateCountVersionsTableFunction"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteCreateAllVersionsTable
    - ExecuteCreateDesiredVersionsTable
    - ExecuteCreateLatestVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateCountVersionsTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateCountVersionsTable/"

  ExecuteCreateLatestDMsTable:
    Type: "Custom::ExecuteCreateLatestDMsTableFunction"
    Condition: WantDMRemoval
    DependsOn:
    - ExecuteClean
    - ExecuteCreateAllVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateLatestDMsTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateLatestDMsTable/"

  ExecuteCreateScenario1Table:
    Type: "Custom::ExecuteCreateScenario1TableFunction"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteCreateAllVersionsTable
    - ExecuteCreateLatestVersionsTable
    - ExecuteCreateDesiredTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario1Table.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario1Table/"

  ExecuteCreateScenario2Table:
    Type: "Custom::ExecuteCreateScenario2TableFunction"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteCreateCountVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario2Table.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario2Table/"

  ExecuteCreateScenario2UndoTable:
    Type: "Custom::ExecuteCreateScenario2UndoTableFunction"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteCreateScenario2Table
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario2UndoTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario2UndoTable/"

  ExecuteCreateScenario3aTable:
    Type: "Custom::ExecuteCreateScenario3aTableFunction"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteCreateChangedKeysTable
    - ExecuteCreateCountVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario3aTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario3aTable/"

  ExecuteCreateScenario3bTable:
    Type: "Custom::ExecuteCreateScenario3bTableFunction"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteCreateChangedKeysTable
    - ExecuteCreateCountVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario3bTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario3bTable/"

  ExecuteCreateScenario3cTable:
    Type: "Custom::ExecuteCreateScenario3cTableFunction"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteCreateChangedKeysTable
    - ExecuteCreateCountVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario3cTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario3cTable/"

  ExecuteCreateScenario4Table:
    Type: "Custom::ExecuteCreateScenario4TableFunction"
    Condition: WantDMRemoval
    DependsOn:
    - ExecuteClean
    - ExecuteCreateLatestDMsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario4Table.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario4Table/"

  ExecuteCreateScenario4UndoTable:
    Type: "Custom::ExecuteCreateScenario4UndoTableFunction"
    Condition: WantDMRemoval
    DependsOn:
    - ExecuteClean
    - ExecuteCreateScenario4Table
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario4UndoTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario4UndoTable/"

  ExecuteCreateScenario5aTable:
    Type: "Custom::ExecuteCreateScenario5aTableFunction"
    Condition: WantCopy
    DependsOn:
    - ExecuteClean
    - ExecuteCreateDesiredVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario5aTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario5aTable/"

  ExecuteCreateScenario5bTable:
    Type: "Custom::ExecuteCreateScenario5bTableFunction"
    Condition: WantCopy
    DependsOn:
    - ExecuteClean
    - ExecuteCreateDesiredVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt QueryExecutor.Arn
      QueryID: !GetAtt CreateScenario5bTable.NamedQueryId
      WorkGroup: !Ref AthenaWorkGroup
      ResultsLocation: !Sub "s3://${AthenaResultsBucket}/Results/CreateScenario5bTable/"

  ExecuteManifestMaker:
    Type: "Custom::ExecuteManifestMakerFunction"
    DependsOn:
    - ExecuteClean
    - ExecuteCreateAllVersionsTable
    Properties:
      ServiceTimeout: 900
      ServiceToken: !GetAtt ManifestMaker.Arn
      SourceBucket: !Ref AthenaResultsBucket
      TableSource: Tables/
      ManifestDestination: Manifests/
      Tables:
        - !If [ WantRollback, "scenario1", !Ref "AWS::NoValue" ]
        - !If [ WantRollback, "scenario2", !Ref "AWS::NoValue" ]
        - !If [ WantRollback, "scenario2_undo", !Ref "AWS::NoValue" ]
        - !If [ WantRollback, "scenario3a", !Ref "AWS::NoValue" ]
        - !If [ WantRollback, "scenario3b", !Ref "AWS::NoValue" ]
        - !If [ WantRollback, "scenario3c", !Ref "AWS::NoValue" ]
        - !If [ WantDMRemoval, "scenario4", !Ref "AWS::NoValue" ]
        - !If [ WantDMRemoval, "scenario4_undo", !Ref "AWS::NoValue" ]
        - !If [ WantCopy, "scenario5a", !Ref "AWS::NoValue" ]
        - !If [ WantCopy, "scenario5b", !Ref "AWS::NoValue" ]
      Dependency:
        - !If [WantRollback, !GetAtt ExecuteCreateScenario1Table.State, !Ref "AWS::NoValue"]
        - !If [WantRollback, !GetAtt ExecuteCreateScenario2Table.State, !Ref "AWS::NoValue"]
        - !If [WantRollback, !GetAtt ExecuteCreateScenario2UndoTable.State, !Ref "AWS::NoValue"]
        - !If [WantRollback, !GetAtt ExecuteCreateScenario3aTable.State, !Ref "AWS::NoValue"]
        - !If [WantRollback, !GetAtt ExecuteCreateScenario3bTable.State, !Ref "AWS::NoValue"]
        - !If [WantRollback, !GetAtt ExecuteCreateScenario3cTable.State, !Ref "AWS::NoValue"]
        - !If [WantDMRemoval, !GetAtt ExecuteCreateScenario4Table.State, !Ref "AWS::NoValue"]
        - !If [WantDMRemoval, !GetAtt ExecuteCreateScenario4UndoTable.State, !Ref "AWS::NoValue"]
        - !If [WantCopy, !GetAtt ExecuteCreateScenario5aTable.State, !Ref "AWS::NoValue"]
        - !If [WantCopy, !GetAtt ExecuteCreateScenario5bTable.State, !Ref "AWS::NoValue"]

  ExecuteS3BatchOpsDeleteCreator1:
    Type: "Custom::ExecuteS3BatchOpsCreator"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteManifestMaker
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt S3BatchOpsCreator.Arn
      ManifestBucket: !Ref AthenaResultsBucket
      ManifestKey: "Manifests/scenario1.csv"
      ReportBucket: !GetAtt AthenaResultsBucket.Arn
      ReportPrefix: "Reports/scenario1"
      Description: "Scenario 1 - Add delete markers"
      FunctionArn: !GetAtt S3BatchOpsDeleteFunction.Arn
      S3BatchOpsRoleArn: !GetAtt S3BatchOpsExecutorRole.Arn
      StartJob: !Ref StartS3BatchOperationsJobs

  ExecuteS3BatchOpsDeleteCreator2:
    Type: "Custom::ExecuteS3BatchOpsCreator"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteManifestMaker
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt S3BatchOpsCreator.Arn
      ManifestBucket: !Ref AthenaResultsBucket
      ManifestKey: "Manifests/scenario2.csv"
      HasVersionIds: True
      ReportBucket: !GetAtt AthenaResultsBucket.Arn
      ReportPrefix: "Reports/scenario2"
      Description: "Scenario 2 - Delete delete markers"
      FunctionArn: !GetAtt S3BatchOpsDeleteFunction.Arn
      S3BatchOpsRoleArn: !GetAtt S3BatchOpsExecutorRole.Arn
      StartJob: !Ref StartS3BatchOperationsJobs

  ExecuteS3BatchOpsCopyCreator3b:
    Type: "Custom::ExecuteS3BatchOpsCreator"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteManifestMaker
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt S3BatchOpsCreator.Arn
      ManifestBucket: !Ref AthenaResultsBucket
      ManifestKey: "Manifests/scenario3b.csv"
      HasVersionIds: True
      ReportBucket: !GetAtt AthenaResultsBucket.Arn
      ReportPrefix: "Reports/scenario3b"
      Description: "Scenario 3b - Restore older object versions"
      DestinationBucket: !Ref BucketName
      StorageClass: !Ref StorageClass
      KMSKey: !If [ WantKMS, !Ref KMSKey, !Ref "AWS::NoValue" ]
      S3BatchOpsRoleArn: !GetAtt S3BatchOpsExecutorRole.Arn
      StartJob: !Ref StartS3BatchOperationsJobs

  ExecuteS3BatchOpsCopyCreator3c:
    Type: "Custom::ExecuteS3BatchOpsCopyCreator"
    Condition: WantRollback
    DependsOn:
    - ExecuteClean
    - ExecuteManifestMaker
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt S3BatchOpsCreator.Arn
      ManifestBucket: !Ref AthenaResultsBucket
      ManifestKey: "Manifests/scenario3c.csv"
      HasVersionIds: True
      ReportBucket: !GetAtt AthenaResultsBucket.Arn
      ReportPrefix: "Reports/scenario3c"
      Description: "Scenario 3c - Restore older object versions > 5GB"
      DestinationBucket: !Ref BucketName
      FunctionArn: !GetAtt S3BatchOpsCopyFunction.Arn
      StorageClass: !Ref StorageClass
      KMSKey: !If [WantKMS, !Ref KMSKey, !Ref "AWS::NoValue" ]
      S3BatchOpsRoleArn: !GetAtt S3BatchOpsExecutorRole.Arn
      StartJob: !Ref StartS3BatchOperationsJobs

  ExecuteS3BatchOpsDeleteCreator4:
    Type: "Custom::ExecuteS3BatchOpsCreator"
    Condition: WantDMRemoval
    DependsOn:
    - ExecuteClean
    - ExecuteManifestMaker
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt S3BatchOpsCreator.Arn
      ManifestBucket: !Ref AthenaResultsBucket
      ManifestKey: "Manifests/scenario4.csv"
      HasVersionIds: True
      ReportBucket: !GetAtt AthenaResultsBucket.Arn
      ReportPrefix: "Reports/scenario4"
      Description: "Scenario 4 - Remove latest DMs"
      FunctionArn: !GetAtt S3BatchOpsDeleteFunction.Arn
      S3BatchOpsRoleArn: !GetAtt S3BatchOpsExecutorRole.Arn
      StartJob: !Ref StartS3BatchOperationsJobs

  ExecuteS3BatchOpsCopyCreator5b:
    Type: "Custom::ExecuteS3BatchOpsCreator"
    Condition: WantCopy
    DependsOn:
    - ExecuteClean
    - ExecuteManifestMaker
    Properties:
      ServiceTimeout: 60
      ServiceToken: !GetAtt S3BatchOpsCreator.Arn
      ManifestBucket: !Ref AthenaResultsBucket
      ManifestKey: "Manifests/scenario5b.csv"
      HasVersionIds: True
      ReportBucket: !GetAtt AthenaResultsBucket.Arn
      ReportPrefix: "Reports/scenario5b"
      Description: "Scenario 5b - Copy objects"
      DestinationBucket: !Ref DestinationBucket
      FunctionArn: !GetAtt S3BatchOpsCopyFunction.Arn
      StorageClass: !Ref StorageClass
      KMSKey: !If [WantKMS, !Ref KMSKey, !Ref "AWS::NoValue" ]
      S3BatchOpsRoleArn: !GetAtt S3BatchOpsExecutorRole.Arn
      StartJob: !Ref StartS3BatchOperationsJobs

Outputs:    
  Manifests:
    Description: Location of the Manifests for S3 Batch Operations
    Value: !Sub "s3://${AthenaResultsBucket}/Manifests/"
 
  S3BatchOperationsDelete1Job:
    Condition: WantRollback
    Description: AWS Management Console link for S3 Batch Operations Delete Job (Scenario 1)
    Value: !GetAtt ExecuteS3BatchOpsDeleteCreator1.Output
    
  S3BatchOperationsDelete2Job:
    Condition: WantRollback
    Description: AWS Management Console link for S3 Batch Operations Delete Job (Scenario 2)
    Value: !GetAtt ExecuteS3BatchOpsDeleteCreator2.Output
    
  S3BatchOperationsCopy3bJob:
    Condition: WantRollback
    Description: AWS Management Console link for S3 Batch Operations Copy Job (Scenario 3b)
    Value: !GetAtt ExecuteS3BatchOpsCopyCreator3b.Output
    
  S3BatchOperationsCopy3cJob:
    Condition: WantRollback
    Description: AWS Management Console link for S3 Batch Operations Copy Job (Scenario 3c)
    Value: !GetAtt ExecuteS3BatchOpsCopyCreator3c.Output

  CopyRole3b:
    Condition: WantRollback
    Description: ARN of the IAM Role used by the S3 Batch Operations Copy Job (Scenario 3b)
    Value: !GetAtt S3BatchOpsExecutorRole.Arn

  CopyRole3c:
    Condition: WantRollback
    Description: ARN of the IAM Role used by the S3 Batch Operations Copy Job (Scenario 3c)
    Value: !GetAtt S3BatchOpsCopyFunctionRole.Arn

  S3BatchOperationsCopy5bJob:
    Condition: WantCopy
    Description: AWS Management Console link for S3 Batch Operations Copy Job (Scenario 5b)
    Value: !GetAtt ExecuteS3BatchOpsCopyCreator5b.Output
    
  CopyRole5b:
    Condition: WantCopy
    Description: ARN of the IAM Role used by the S3 Batch Operations Copy Job (Scenario 5b)
    Value: !GetAtt S3BatchOpsExecutorRole.Arn

  S3BatchOperationsDelete4Job:
    Condition: WantDMRemoval
    Description: AWS Management Console link for S3 Batch Operations Delete Job (Scenario 4)
    Value: !GetAtt ExecuteS3BatchOpsDeleteCreator4.Output

  InventorySource:
    Description: Where was the bucket inventory read from?
    Value: !If [ CreateInventory, !GetAtt ExecuteInventoryFinder.InventorySource, !Ref InventoryCSVLocation ]